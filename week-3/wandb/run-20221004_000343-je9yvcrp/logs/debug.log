2022-10-04 00:03:43,066 INFO    MainThread:81754 [wandb_setup.py:_flush():68] Configure stats pid to 81754
2022-10-04 00:03:43,067 INFO    MainThread:81754 [wandb_setup.py:_flush():68] Loading settings from /Users/ihork/.config/wandb/settings
2022-10-04 00:03:43,067 INFO    MainThread:81754 [wandb_setup.py:_flush():68] Loading settings from /Users/ihork/Programs/studing/projector/ML-in-production/week-3/wandb/settings
2022-10-04 00:03:43,067 INFO    MainThread:81754 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
2022-10-04 00:03:43,067 INFO    MainThread:81754 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program': '<python with no main file>'}
2022-10-04 00:03:43,067 INFO    MainThread:81754 [wandb_init.py:_log_setup():476] Logging user logs to /Users/ihork/Programs/studing/projector/ML-in-production/week-3/wandb/run-20221004_000343-je9yvcrp/logs/debug.log
2022-10-04 00:03:43,067 INFO    MainThread:81754 [wandb_init.py:_log_setup():477] Logging internal logs to /Users/ihork/Programs/studing/projector/ML-in-production/week-3/wandb/run-20221004_000343-je9yvcrp/logs/debug-internal.log
2022-10-04 00:03:43,068 INFO    MainThread:81754 [wandb_init.py:init():516] calling init triggers
2022-10-04 00:03:43,068 INFO    MainThread:81754 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
config: {}
2022-10-04 00:03:43,068 INFO    MainThread:81754 [wandb_init.py:init():569] starting backend
2022-10-04 00:03:43,068 INFO    MainThread:81754 [wandb_init.py:init():573] setting up manager
2022-10-04 00:03:43,081 INFO    MainThread:81754 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
2022-10-04 00:03:43,085 INFO    MainThread:81754 [wandb_init.py:init():580] backend started and connected
2022-10-04 00:03:43,087 INFO    MainThread:81754 [wandb_init.py:init():658] updated telemetry
2022-10-04 00:03:43,114 INFO    MainThread:81754 [wandb_init.py:init():692] communicating run to backend with 30 second timeout
